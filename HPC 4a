HPC 4a

Title:
Write a CUDA Program for Addition of two large vectors

1
!nvcc --version

2
code = """
#include <iostream>
#include <cuda.h>

__global__
void vectorAddition(const int* A, const int* B, int* C, int size) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;

    if (tid < size) {
        C[tid] = A[tid] + B[tid];
    }
}

int main() {
    int size = 100;  // Size of the vectors
    int* A, * B, * C;    // Host vectors
    int* d_A, * d_B, * d_C;  // Device vectors

    // Allocate memory for host vectors
    A = new int[size];
    B = new int[size];
    C = new int[size];

    // Initialize input vectors
    for (int i = 0; i < size; ++i) {
        A[i] = 1;
        B[i] = 2;
    }

    // Allocate memory for device vectors
    cudaMalloc((void**)&d_A, size * sizeof(int));
    cudaMalloc((void**)&d_B, size * sizeof(int));
    cudaMalloc((void**)&d_C, size * sizeof(int));

    // Copy input vectors from host to device
    cudaMemcpy(d_A, A, size * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, B, size * sizeof(int), cudaMemcpyHostToDevice);

    // Define the number of threads per block and the number of blocks
    int threadsPerBlock = 256;
    int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;

    // Launch the vector addition kernel
    vectorAddition<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, size);

    // Copy the result from the device to the host
    cudaMemcpy(C, d_C, size * sizeof(int), cudaMemcpyDeviceToHost);

    // Print the result
    for (int i = 0; i < size; ++i) {
       std:: cout << C[i] << " ";
    }
    std::cout <<"success "<< std::endl;

    // Free device memory
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);

    // Free host memory
    delete[] A;
    delete[] B;
    delete[] C;

    return 0;
}

"""

4
!nvcc assign1.cu

5
!./a.out #executable file to exceute

6
!nvprof ./a.out


---------------------------------------
ALternative:: 
---------------------------------------
#include<stdio.h>
#include<iostream>
#include<cstdlib>
//*important to add following library to allow a programmer to use parallel paradigms**
#include<omp.h>	
using namespace std;
#define MAX 100

int main()
{
  int a[MAX],b[MAX],c[MAX],i;
  printf("\n First Vector:\t");
  //Instruct a master thread to fork and generate more threads to process following loop structure
  #pragma omp parallel for
  for(i=0;i<MAX;i++)
  {
     a[i]=rand()%1000;
  }
  //Discuss issue of this for loop below-if we make it parallel, possibly values that get printed will not be in sequence as we dont have any control on order of threads execution
  for(i=0;i<MAX;i++)
  {
     printf("%d\t",a[i]);
  }
  printf("\n Second Vector:\t");
  #pragma omp parallel for
  for(i=0;i<MAX;i++)
  {
     b[i]=rand()%1000;
  }
  for(i=0;i<MAX;i++)
  {
     printf("%d\t",b[i]);
  }
  printf("\n Parallel-Vector Addition:(a,b,c)\t");
  #pragma omp parallel for
  for(i=0;i<MAX;i++)
  {
     c[i]=a[i]+b[i];
  }
  for(i=0;i<MAX;i++)
  {
     printf("\n%d\t%d\t%d",a[i],b[i],c[i]);
  }
}

-------------------------------------------------------------------------------------------------------------------------------------------------

What is CUDA
CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA. It allows developers to use the power of NVIDIA graphics processing units (GPUs) to accelerate computation tasks in various applications, including scientific computing, machine learning, and computer vision.CUDA provides a set of programming APIs, libraries, and tools that enable developers to write and execute parallel code on NVIDIA GPUs. It supports popular programming languages like C, C++, and Python, and provides a simple programming model that abstracts away much of the low-level details of GPU architecture.
Using CUDA, developers can exploit the massive parallelism and high computational power of GPUs to accelerate computationally intensive tasks, such as matrix operations, image processing, and deep learning. CUDA has become an important tool for scientific research and is widely used in fields like physics, chemistry, biology, and engineering.

Steps for Addition of two large vectors using CUDA
1. Define the size of the vectors: In this step, you need to define the size of the vectors that you want to
add. This will determine the number of threads and blocks you will need to use to parallelize the
addition operation.
2. Allocate memory on the host: In this step, you need to allocate memory on the host for the two
vectors that you want to add and for the result vector. You can use the C malloc function to allocate
memory.
3. Initialize the vectors: In this step, you need to initialize the two vectors that you want to add on the
host. You can use a loop to fill the vectors with data.
4. Allocate memory on the device: In this step, you need to allocate memory on the device for the two
vectors that you want to add and for the result vector. You can use the CUDA function cudaMalloc to
allocate memory.
5. Copy the input vectors from host to device: In this step, you need to copy the two input vectors from
the host to the device memory. You can use the CUDA function cudaMemcpy to copy the vectors.
6. Launch the kernel: In this step, you need to launch the CUDA kernel that will perform the addition operation. The kernel will be executed by multiple threads in parallel. You can use the <<<...>>>
syntax to specify the number of blocks and threads to use.
7. Copy the result vector from device to host: In this step, you need to copy the result vector from the
device memory to the host memory. You can use the CUDA function cudaMemcpy to copy the result vector.

8. Free memory on the device: In this step, you need to free the memory that was allocated on the device. You can use the CUDA function cudaFree to free the memory.

9. Free memory on the host: In this step, you need to free the memory that was allocated on the host. You can use the C free function to free the memory.

Execution of Program over CUDA Environment
Here are the steps to run a CUDA program for adding two large vectors:
1. Install CUDA Toolkit: First, you need to install the CUDA Toolkit on your system. You can
download the CUDA Toolkit from the NVIDIA website and follow the installation instructions
provided.

2. Set up CUDA environment: Once the CUDA Toolkit is installed, you need to set up the CUDA
environment on your system. This involves setting the PATH and LD_LIBRARY_PATH
environment variables to the appropriate directories.

3. Write the CUDA program: You need to write a CUDA program that performs the addition of two
large vectors. You can use a text editor to write the program and save it with a .cu extension.

4. Compile the CUDA program: You need to compile the CUDA program using the nvcc compiler that
comes with the CUDA Toolkit. The command to compile the program is:
nvcc -o program_name program_name.cu

5. This will generate an executable program named program_name.
Run the CUDA program: Finally, you can run the CUDA program by executing the executable file generated in the previous step. The command to run the program is: ./program_name
This will execute the program and perform the addition of two large vectors.
