DL A2
Title: Binary classification using Deep Neural Networks Example: Classify movie reviews into positive" reviews and "negative" reviews, just based on the text content of the reviews. Use IMDB dataset

Aim: 
To implement movie reviews into positive" reviews and "negative" reviews, just based on the text content of the reviews.

from keras.datasets import imdb
%matplotlib inline
import numpy as np
import pandas as pd
from matplotlib import cm
import matplotlib.pyplot as plt
import seaborn as sns
import os
import time

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding
from keras.layers import Conv1D, GlobalMaxPooling1D
from keras.callbacks import EarlyStopping
from keras import models

(X_train, y_train), (X_test, y_test) = imdb.load_data()
X = np.concatenate((X_train, X_test), axis=0)
y = np.concatenate((y_train, y_test), axis=0)

##training data shape review
print("Training data: ")
print(X.shape)
print(y.shape)
print("Classes: ")
print(np.unique(y))

print("Number of words: ")
print(len(np.unique(np.hstack(X))))

print("Review length: ")
result = [len(x) for x in X]
print("Mean %.2f words (%f)" % (np.mean(result), np.std(result)))
# plot review length
plt.boxplot(result)
plt.show()

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=5000)

def vectorize_sequences(sequences, dimension=5000):
    # Create an all-zero matrix of shape (len(sequences), dimension)
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.  # set specific indices of results[i] to 1s
    return results

# Our vectorized training data
x_train = vectorize_sequences(train_data)
# Our vectorized test data
x_test = vectorize_sequences(test_data)

# Our vectorized labels one-hot encoder
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')

from keras import layers
from keras import models
model = models.Sequential()
model.add(layers.Dense(32, activation='relu', input_shape=(5000,)))
model.add(layers.Dense(32, activation='relu',))
model.add(layers.Dense(1, activation='sigmoid'))

#Set validation set aside
x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['acc'])
              
start_time_m1 = time.time()
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
total_time_m1 = time.time() - start_time_m1

print("The Dense Convolutional Neural Network 1 layer took %.4f seconds to train." % (total_time_m1))

history_dict = history.history
history_dict.keys()

import matplotlib.pyplot as plt
%matplotlib inline
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.clf()   # clear figure
acc_values = history_dict['acc']
val_acc_values = history_dict['val_acc']
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

model.summary()

from sklearn.metrics import confusion_matrix, accuracy_score, auc
#predictions
pred = model.predict(x_test)
classes_x=np.argmax(pred,axis=1)
#accuracy
accuracy_score(y_test,classes_x)

#Confusion Matrix
conf_mat = confusion_matrix(y_test, classes_x)
print(conf_mat)
conf_mat_normalized = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]
sns.heatmap(conf_mat_normalized)
plt.ylabel('True label')
plt.xlabel('Predicted label')

#Dense with Two Layer
model2 = models.Sequential()
model2.add(layers.Dense(32, activation='relu', input_shape=(5000,)))
model2.add(layers.Dense(32, activation='relu'))
model2.add(layers.Dense(32, activation='relu'))
model2.add(layers.Dense(1, activation='sigmoid'))

model2.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['acc'])
              
start_time_m2 = time.time()
history= model2.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
total_time_m2 = time.time() - start_time_m2
print("The Dense Convolutional Neural Network 2 layers took %.4f seconds to train." % (total_time_m2))

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('DNN 2 layer Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.clf()   # clear figure
acc_values = history_dict['acc']
val_acc_values = history_dict['val_acc']
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('DNN 2 layer Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

model2.summary()

from numpy.ma.core import argmax
pred = model2.predict(x_test)
classes_x=argmax(pred,axis=-1)
#accuracy
accuracy_score(y_test,classes_x)

-------------------------------------------------------------------------------------------------------------------------------------------------

Theory:
Binary Classification refers to classifying samples in one of two categories. We will design a neural network to perform two-class classification, or binaryclassification, of reviews, from the IMDB movie reviews dataset, to determine whether the reviews are positive or negative. We will use the Python library, Keras.

THE IMDB DATASET
The IMDB dataset is a set of 50,000 highly polarized reviews from the Internet Movie Database. They are split into 25000 reviews each for training and testing.Each set contains an equal number (50%) of positive and negative reviews.
The IMDB dataset comes packaged with Keras. It consists of reviews and their corresponding labels (0 for negative and 1 for positive review). The reviews are asequence of words. They come preprocessed as a sequence of integers, where eachinteger stands for a specific word in the dictionary.

The IMDB dataset can be loaded directly from Keras and will usually downloadabout 80 MB on your machine.

LOADING THE DATA
Let’s load the prepackaged data from Keras. We will only include 10,000 of the most frequently occurring words.

PREPARING THE DATA
We cannot feed a list of integers into our deep neural network. We will need to convert them into tensors.
To prepare our data, we will One-hot Encode our lists and turn them into vectorsof 0’s and 1’s. This would blow up all of our sequences into 10,000-dimensionalvectors containing 1 at all indices corresponding to integers present in that sequence. This vector will have element 0 at all index, which is not present in theinteger sequence.
Simply put, the 10,000-dimensional vector corresponding to each review will have
 Every index corresponding to a word
 Every index with value 1, is a word that is present in the review and is denotedby its integer counterpart.
 Every index containing 0 is a word not present in the review.
We will vectorize our data manually for maximum clarity. This will result in a tensor of shape
(25000, 10000).

Model Architecturewe will use
1. Two intermediate layers with 16 hidden units each
2. Thirdl ayer that will out put the scalar sentiment prediction
3. Intermediate layers will use the relu activation function. relu or Rectified linear unit function will zero out the negative values.
4. Sigmoid activation for the final layer or output layer. A sigmoid function “squashes” arbitrary values into the [0,1] range.

Compiling the model
In this step, we will choose an optimizer, a loss function, and metrics to observe. We will go forward with
 binary_crossentropy loss function, commonly used for Binary Classification
 rmsprop optimizer and
 accuracy as a measure of performance
We can pass our choices for optimizer, loss function and metrics as strings to the compile function because rmsprop, binary_crossentropy and accuracy come packaged with Keras.

Conclusion
And thus, we have successfully classified reviews on IMDB.
